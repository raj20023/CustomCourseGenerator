{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9520a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A clever introduction! I love it.\\n\\nAlright, let's break down what LangChain might be:\\n\\n1. **Lang**: This prefix suggests that LangChain has something to do with language. Perhaps it's a tool or technology related to natural language processing (NLP), linguistics, or computer science.\\n2. **Chain**: This word implies connection, continuity, or a series of elements linked together. In the context of language, this might refer to processes, algorithms, or systems that work together to achieve a specific goal.\\n\\nCombining these insights, I'll take a wild guess:\\n\\nLangChain could be a type of NLP framework or system that enables the creation of longer, more coherent text sequences by chaining together individual language processing tasks or modules. This might include techniques like language modeling, syntax analysis, semantic reasoning, and context-aware generation.\\n\\nAm I on the right track?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76c445d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from datetime import datetime\n",
    "from typing import Annotated, TypedDict, Union\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@tool\n",
    "def get_now(format: str = \"%Y-%m-%d %H:%M:%S\"):\n",
    "    \"\"\"\n",
    "    Get the current time\n",
    "    \"\"\"\n",
    "    return datetime.now().strftime(format)\n",
    "\n",
    "\n",
    "tools = [get_now]\n",
    "\n",
    "tool_executor = ToolNode(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7851fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajp2\\OneDrive\\Desktop\\Learning\\MultiAgent\\env\\Lib\\site-packages\\langsmith\\client.py:280: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================= input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'} template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    chat_history: list[BaseMessage]\n",
    "    agent_outcome: Union[AgentAction, AgentFinish, None]\n",
    "    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n",
    "\n",
    "\n",
    "model = ChatOllama(model=\"openhermes\")\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "print(\"=================\", prompt)\n",
    "\n",
    "agent_runnable = create_react_agent(model, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26aca43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tools(state):\n",
    "    print(\"Called `execute_tools`\")\n",
    "    messages = [state[\"agent_outcome\"]]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    tool_name = last_message.tool\n",
    "\n",
    "    print(f\"Calling tool: {tool_name}\")\n",
    "\n",
    "    action = ToolNode(\n",
    "        name=tool_name,\n",
    "        messages_key=last_message.tool_input,\n",
    "    )\n",
    "    response = tool_executor.invoke(action)\n",
    "    return {\"intermediate_steps\": [(state[\"agent_outcome\"], response)]}\n",
    "\n",
    "\n",
    "def run_agent(state):\n",
    "    \"\"\"\n",
    "    #if you want to better manages intermediate steps\n",
    "    inputs = state.copy()\n",
    "    if len(inputs['intermediate_steps']) > 5:\n",
    "        inputs['intermediate_steps'] = inputs['intermediate_steps'][-5:]\n",
    "    \"\"\"\n",
    "    agent_outcome = agent_runnable.invoke(state)\n",
    "    return {\"agent_outcome\": agent_outcome}\n",
    "\n",
    "\n",
    "def should_continue(state):\n",
    "    messages = [state[\"agent_outcome\"]]\n",
    "    last_message = messages[-1]\n",
    "    if \"Action\" not in last_message.log:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"continue\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e04563",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", run_agent)\n",
    "workflow.add_node(\"action\", execute_tools)\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\", should_continue, {\"continue\": \"action\", \"end\": END}\n",
    ")\n",
    "\n",
    "\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "app = workflow.compile()\n",
    "\n",
    "input_text = \"Whats the current time?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e04aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_outcome': AgentAction(tool=\"Use `get_now` with format '%Y-%m-%d %H:%M:%S'\", tool_input='%Y-%m-%d %H:%M:%S (Year-month-day Hour:Minute:Second)', log=\"To get the current time, we will use the `get_now` tool. This tool allows us to obtain the current time in a specific format.\\n\\nAction: Use `get_now` with format '%Y-%m-%d %H:%M:%S'\\nAction Input: %Y-%m-%d %H:%M:%S (Year-month-day Hour:Minute:Second)\")}\n",
      "Called `execute_tools`\n",
      "Calling tool: Use `get_now` with format '%Y-%m-%d %H:%M:%S'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ToolNode.__init__() missing 1 required positional argument: 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m inputs = {\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: input_text, \u001b[33m\"\u001b[39m\u001b[33mchat_history\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m      2\u001b[39m results = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajp2\\OneDrive\\Desktop\\Learning\\MultiAgent\\env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2377\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2371\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2372\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2373\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2374\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2375\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2376\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2382\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2383\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2384\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2385\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajp2\\OneDrive\\Desktop\\Learning\\MultiAgent\\env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:158\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    156\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajp2\\OneDrive\\Desktop\\Learning\\MultiAgent\\env\\Lib\\site-packages\\langgraph\\pregel\\retry.py:39\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     37\u001b[39m     task.writes.clear()\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     41\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajp2\\OneDrive\\Desktop\\Learning\\MultiAgent\\env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:622\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajp2\\OneDrive\\Desktop\\Learning\\MultiAgent\\env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:376\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mexecute_tools\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      6\u001b[39m tool_name = last_message.tool\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling tool: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m action = \u001b[43mToolNode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast_message\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m response = tool_executor.invoke(action)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mintermediate_steps\u001b[39m\u001b[33m\"\u001b[39m: [(state[\u001b[33m\"\u001b[39m\u001b[33magent_outcome\u001b[39m\u001b[33m\"\u001b[39m], response)]}\n",
      "\u001b[31mTypeError\u001b[39m: ToolNode.__init__() missing 1 required positional argument: 'tools'",
      "During task with name 'action' and id 'f2697b36-9c90-e7e0-f7dd-82e24f7570f1'"
     ]
    }
   ],
   "source": [
    "inputs = {\"input\": input_text, \"chat_history\": []}\n",
    "results = []\n",
    "for s in app.stream(inputs):\n",
    "    result = list(s.values())[0]\n",
    "    results.append(result)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d96ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7cd477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajp2\\AppData\\Local\\Temp\\ipykernel_23644\\2441345325.py:9: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"llama3\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x19a9dd7e570>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [chain.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55aca000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "graph = graph_builder.compile()\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4bdb89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: A HumanMessage!\n",
      "\n",
      "Let's break it down:\n",
      "\n",
      "* `content='langchin'`: This is the content of the message, which appears to be a string containing a language code or name.\n",
      "* `additional_kwargs={}`: This is an empty dictionary that likely contains additional keyword arguments (kwargs) for this message. Since it's empty, there are no specific key-value pairs.\n",
      "* `response_metadata={}`: Another empty dictionary, this one for response metadata related to the message. No specific information stored here either.\n",
      "* `id='26611c1a-db93-4161-8453-25fceb7498a2'`: A unique identifier for this HumanMessage instance.\n",
      "\n",
      "So, in essence, we have a message with some basic information about its content and ID, but no additional metadata or specific data provided.\n",
      "Assistant: It looks like we have a question in the format of a HumanMessage object, which is used to pass messages between humans and AI models.\n",
      "\n",
      "Let's break it down:\n",
      "\n",
      "* `content`: The content of the message is set to `'langchain'`, which suggests that this message is related to language translation or generation.\n",
      "* `additional_kwargs`: This dictionary is empty, so there are no additional keyword arguments passed with the message.\n",
      "* `response_metadata`: Also an empty dictionary, this contains metadata about the response, such as timestamps or IDs.\n",
      "* `id`: The ID of the message is set to `'1bedae42-ddd0-4450-862d-b2f83901d2a6'`, which can be used to uniquely identify the message.\n",
      "\n",
      "Now that we've analyzed the message, what would you like to do? Would you like me to respond with a translation or some other information related to language generation?\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7320f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# setup the simple tools using LangChain tool decorator\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def square(a: int) -> int:\n",
    "    \"\"\"Calculates the square of a number.\"\"\"\n",
    "    a = int(a)\n",
    "    return a * a\n",
    "\n",
    "# setup the toolkit\n",
    "toolkit = [add, multiply, square]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c0cb809",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      6\u001b[39m system_prompt = \u001b[33m\"\"\"\u001b[39m\u001b[33m You are a mathematical assistant.\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m        Use your tools to answer questions. If you do not have a tool to\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m        answer the question, say so. \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     10\u001b[39m tool_calling_prompt = ChatPromptTemplate.from_messages(\n\u001b[32m     11\u001b[39m     [\n\u001b[32m     12\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, system_prompt),\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     ]\n\u001b[32m     17\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m tool_runnable = \u001b[43mcreate_tool_calling_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolkit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_calling_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m tool_actions = tool_runnable.invoke({\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhi! what\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms 1+1 and  2 times 2\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mintermediate_steps\u001b[39m\u001b[33m'\u001b[39m: []})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajp2\\OneDrive\\Desktop\\Learning\\MultiAgent\\env\\Lib\\site-packages\\langchain\\agents\\tool_calling_agent\\base.py:99\u001b[39m, in \u001b[36mcreate_tool_calling_agent\u001b[39m\u001b[34m(llm, tools, prompt, message_formatter)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(llm, \u001b[33m\"\u001b[39m\u001b[33mbind_tools\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis function requires a .bind_tools method be implemented on the LLM.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     98\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m llm_with_tools = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m agent = (\n\u001b[32m    102\u001b[39m     RunnablePassthrough.assign(\n\u001b[32m    103\u001b[39m         agent_scratchpad=\u001b[38;5;28;01mlambda\u001b[39;00m x: message_formatter(x[\u001b[33m\"\u001b[39m\u001b[33mintermediate_steps\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m     | ToolsAgentOutputParser()\n\u001b[32m    108\u001b[39m )\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m agent\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rajp2\\OneDrive\\Desktop\\Learning\\MultiAgent\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1325\u001b[39m, in \u001b[36mBaseChatModel.bind_tools\u001b[39m\u001b[34m(self, tools, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind_tools\u001b[39m(\n\u001b[32m   1308\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1309\u001b[39m     tools: Sequence[\n\u001b[32m   (...)\u001b[39m\u001b[32m   1314\u001b[39m     **kwargs: Any,\n\u001b[32m   1315\u001b[39m ) -> Runnable[LanguageModelInput, BaseMessage]:\n\u001b[32m   1316\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Bind tools to the model.\u001b[39;00m\n\u001b[32m   1317\u001b[39m \n\u001b[32m   1318\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1323\u001b[39m \u001b[33;03m        A Runnable that returns a message.\u001b[39;00m\n\u001b[32m   1324\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "#define system prompt for tool calling agent\n",
    "system_prompt = \"\"\" You are a mathematical assistant.\n",
    "        Use your tools to answer questions. If you do not have a tool to\n",
    "        answer the question, say so. \"\"\"\n",
    "\n",
    "tool_calling_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tool_runnable = create_tool_calling_agent(model, toolkit, prompt  = tool_calling_prompt)\n",
    "tool_actions = tool_runnable.invoke({\"input\": \"hi! what's 1+1 and  2 times 2\", 'intermediate_steps': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eef1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import (\n",
    "    StreamingStdOutCallbackHandler\n",
    ")\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "from tools.nslookup_custom_tool import NslookupTool\n",
    "\n",
    "\n",
    "\n",
    "llm = Ollama(base_url=\"http://localhost:11434\", \n",
    "             model=\"llama2:13b\", \n",
    "             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "             temperature = 0\n",
    "             )\n",
    "\n",
    "nslookup_tool = NslookupTool()\n",
    "tools = [\n",
    "    StructuredTool.from_function(\n",
    "        func=nslookup_tool.run,\n",
    "        name=\"Nslookup\",\n",
    "        description=\"Useful for querying DNS to obtain domain name or IP address mapping, as well as other DNS records. Input: IP address or domain name.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "res = agent.run(\"Do nslookup to google.com, what is google.com ip address?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bee89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_1': 'Design course', 'detail_1': 'Outline modules'}\n"
     ]
    }
   ],
   "source": [
    "# Problem: Model returns JSON-like output with single quotes, which is not valid JSON\n",
    "# Goal: Ensure model returns output strictly with double quotes for JSON parsing\n",
    "\n",
    "# Suggestion 1: Enforce JSON schema parsing using pydantic with validation after model output\n",
    "# Suggestion 2: Use stricter post-processing or LLM configuration to reject malformed output\n",
    "\n",
    "# Example workaround using Python to sanitize LLM output with single quotes\n",
    "import json\n",
    "import ast\n",
    "from typing import Union\n",
    "\n",
    "def safe_parse_json(response_text: str) -> Union[dict, None]:\n",
    "    try:\n",
    "        # Try parsing directly as JSON\n",
    "        return json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            # Try using ast.literal_eval if single quotes are used\n",
    "            parsed = ast.literal_eval(response_text)\n",
    "            # Re-dump to proper JSON string with double quotes\n",
    "            return json.loads(json.dumps(parsed))\n",
    "        except Exception as e:\n",
    "            print(\"Parsing failed:\", e)\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "response = \"\"\"{ 'task_1': \"Design course\", 'detail_1': 'Outline modules' }\"\"\"\n",
    "parsed = safe_parse_json(response)\n",
    "print(parsed)\n",
    "\n",
    "# Additional fix: Append this instruction to every prompt to reduce risk\n",
    "# \"Ensure every string in the JSON output is enclosed in double quotes (\\\"\\\"), not single quotes (\\'\\').\"\n",
    "\n",
    "# Note: LLMs like ChatGPT may still hallucinate formats, so always sanitize before processing.\n",
    "\n",
    "# If using LangChain or Ollama with JSONOutputParser, you might wrap the parsing call like this:\n",
    "# try:\n",
    "#     parsed_output = your_parser.invoke(output)\n",
    "# except Exception as e:\n",
    "#     parsed_output = safe_parse_json(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f8c432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
